{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from loss_functions import DiceLoss  #, FocalLoss\n",
    "from models_multiple_GPUs import *\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we test some CNN architectures for training on the heat plume prediction dataset. We start by defining some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(unet, savepath, epoch_number, train_dataset, val_dataset):\n",
    "    \"\"\"\n",
    "    This function plots two rows: one visualizing the results for the training image and one visualizing \n",
    "    the result for the validation image.\n",
    "    \"\"\"\n",
    "    def plot_subplot(position, image, title='', vmin=None, vmax=None):\n",
    "        plt.subplot(4, 3, position)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image, cmap=\"RdBu_r\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "    def process_and_plot(images, masks, start_pos):\n",
    "        unet.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = unet([img.unsqueeze(0) for img in images]).cpu()\n",
    "            full_images = unet.concatenate_tensors([img.unsqueeze(0) for img in images]).squeeze().cpu()\n",
    "\n",
    "        for i in range(3):\n",
    "            plot_subplot(start_pos + i, full_images[i].cpu())\n",
    "        \n",
    "        plot_subplot(start_pos + 3, predictions[0, 0].cpu(), vmin=0, vmax=1)\n",
    "        plot_subplot(start_pos + 4, masks.cpu()[0])\n",
    "        plot_subplot(start_pos + 5, torch.abs(masks.cpu()[0] - predictions[0, 0].cpu()))\n",
    "\n",
    "    plt.figure(figsize=(9, 12))\n",
    "    \n",
    "    # Adjust spacing between plots\n",
    "    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "    train_image, train_mask = train_dataset[0]\n",
    "    process_and_plot(train_image, train_mask, 1)\n",
    "\n",
    "    val_image, val_mask = val_dataset[0]\n",
    "    process_and_plot(val_image, val_mask, 7)\n",
    "\n",
    "    os.makedirs(os.path.join(savepath, \"figures\"), exist_ok=True)\n",
    "    plt.savefig(os.path.join(savepath, \"figures\", f\"epoch_{epoch_number}.png\"), bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMultipleGPUs(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to load images and their corresponding masks, apply transformations,\n",
    "    and handle data augmentation. Supports splitting images into subdomains for \n",
    "    multi-GPU training.\n",
    "\n",
    "    Attributes:\n",
    "        img_labels (list): List of image filenames.\n",
    "        img_dir (str): Directory containing images.\n",
    "        mask_dir (str): Directory containing masks.\n",
    "        transform (callable, optional): Transformation function for images.\n",
    "        target_transform (callable, optional): Transformation function for masks.\n",
    "        data_augmentation (callable, optional): Data augmentation function.\n",
    "        size (int, optional): Size of the images.\n",
    "        patch_size (int, optional): Size of the patches to crop from the images.\n",
    "        subdomains_dist (tuple, optional): Distribution of subdomains (rows, cols).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_labels, image_dir, mask_dir, transform=None, target_transform=None, \n",
    "                 data_augmentation=None, size=2560, patch_size=1280, subdomains_dist=(2, 2)):\n",
    "        self.img_labels = image_labels\n",
    "        self.img_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.size = size\n",
    "        self.patch_size = patch_size\n",
    "        self.subdomains_dist = subdomains_dist\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __split_image(self, full_image):\n",
    "        \"\"\"\n",
    "        Split the image into subdomains based on subdomains_dist.\n",
    "        \"\"\"\n",
    "        subdomain_tensors = []\n",
    "        subdomain_height = full_image.shape[1] // self.subdomains_dist[0]\n",
    "        subdomain_width = full_image.shape[2] // self.subdomains_dist[1]\n",
    "\n",
    "        for i in range(self.subdomains_dist[0]):\n",
    "            for j in range(self.subdomains_dist[1]):\n",
    "                subdomain = full_image[:, \n",
    "                                       i * subdomain_height: (i + 1) * subdomain_height,\n",
    "                                       j * subdomain_width: (j + 1) * subdomain_width]\n",
    "                subdomain_tensors.append(subdomain)\n",
    "\n",
    "        return subdomain_tensors        \n",
    "\n",
    "    def __crop_patch(self, full_image, full_mask):\n",
    "        \"\"\"\n",
    "        Crop a patch from the full image and mask.\n",
    "        \"\"\"\n",
    "        _, height, width = full_image.shape\n",
    "        patch_height, patch_width = self.patch_size, self.patch_size\n",
    "\n",
    "        if height < patch_height or width < patch_width:\n",
    "            raise ValueError(\"Patch size must be smaller than image size.\")\n",
    "        \n",
    "        top = random.randint(0, height - patch_height)\n",
    "        left = random.randint(0, width - patch_width)\n",
    "        \n",
    "        image_patch = full_image[:, top:top + patch_height, left:left + patch_width]\n",
    "        mask_patch = full_mask[:, top:top + patch_height, left:left + patch_width]\n",
    "\n",
    "        return image_patch, mask_patch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_labels[idx]\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}\")                \n",
    "        mask_path = os.path.join(self.mask_dir, f\"{img_name}\")\n",
    "\n",
    "        image = torch.load(img_path)\n",
    "        mask = torch.load(mask_path)\n",
    "\n",
    "        image, mask = self.__crop_patch(image, mask)\n",
    "\n",
    "        if self.data_augmentation:\n",
    "            image, mask = self.data_augmentation(image, mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "            \n",
    "        images = self.__split_image(image)\n",
    "\n",
    "        return images, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomains_dist = (1,1)\n",
    "image_dir = os.path.join(\"data\", \"Inputs\")\n",
    "mask_dir = os.path.join(\"data\", \"Labels\")\n",
    "patch_size = 1280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "train_dataset = DatasetMultipleGPUs(image_labels=[\"RUN_1.pt\"], image_dir=image_dir, mask_dir=mask_dir, transform=None,\n",
    "                                    target_transform=None, data_augmentation=None, patch_size=patch_size, subdomains_dist=subdomains_dist)\n",
    "\n",
    "val_dataset = DatasetMultipleGPUs(image_labels=[\"RUN_2.pt\"], image_dir=image_dir, mask_dir=mask_dir, transform=None,\n",
    "                                    target_transform=None, data_augmentation=None, patch_size=patch_size, subdomains_dist=subdomains_dist)\n",
    "\n",
    "test_dataset = DatasetMultipleGPUs(image_labels=[\"RUN_4.pt\"], image_dir=image_dir, mask_dir=mask_dir, transform=None,\n",
    "                                    target_transform=None, data_augmentation=None, patch_size=patch_size, subdomains_dist=subdomains_dist)\n",
    "\n",
    "# Train hyperparams\n",
    "batch_size = 1\n",
    "batch_size_test = 1\n",
    "\n",
    "# Define dataloaders\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) #, num_workers=6)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=batch_size_test, shuffle=False)# , num_workers=6)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)# , num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.up = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_blocks, channels):\n",
    "        super(FCN, self).__init__()\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        \n",
    "        # Encoder\n",
    "        in_channels = 3  # Assuming input is RGB images\n",
    "        for out_channels in channels[:num_blocks]:\n",
    "            self.encoder_blocks.append(EncoderBlock(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Decoder\n",
    "        for out_channels in channels[num_blocks:]:\n",
    "            self.decoder_blocks.append(DecoderBlock(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoder_outputs = []\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "            encoder_outputs.append(x)\n",
    "\n",
    "        # Decoder\n",
    "        for i, block in enumerate(self.decoder_blocks):\n",
    "            x = block(x)\n",
    "            if i < len(self.encoder_blocks) - 1:\n",
    "                x = x + encoder_outputs[-(i + 2)]  # Skip connection\n",
    "                \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 16, 640, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 16, 640, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 16, 640, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 32, 320, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "Validation Loss: 0.0771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 16, 640, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 32, 320, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "Validation Loss: 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 16, 640, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 32, 320, 320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "Validation Loss: 0.0809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 640, 640])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 128, 80, 80])\n",
      "torch.Size([1, 64, 160, 160])\n",
      "torch.Size([1, 32, 320, 320])\n",
      "torch.Size([1, 16, 640, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/1 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_blocks = 4\n",
    "channels = [16, 32, 64, 128, 64, 32, 16, 1]\n",
    "model = FCN(num_blocks, channels)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in tqdm(dataloader_train, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        inputs, targets = inputs[0].to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader_train.dataset)\n",
    "    print(f'Training Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader_val, desc=f'Validation'):\n",
    "            inputs, targets = inputs[0].to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(dataloader_val.dataset)\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(dataloader_test, desc=f'Testing'):\n",
    "        inputs, targets = inputs[0].to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "test_loss /= len(dataloader_test.dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
